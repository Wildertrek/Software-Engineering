{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests\n",
    "We want to test our functions in a way that is repeatable and automated. Ideally, we'd run a test program that runs all our unit tests and cleanly lets us know which ones failed and which ones succeeded. Fortunately, there are great tools available in Python that we can use to create effective unit tests!\n",
    "\n",
    "Unit Test Advantages and Disadvantages\n",
    "The advantage of unit tests is that they are isolated from the rest of your program, and thus, no dependencies are involved. They don't require access to databases, APIs, or other external sources of information. However, passing unit tests isn’t always enough to prove that our program is working successfully. To show that all the parts of our program work with each other properly, communicating and transferring data between them correctly, we use integration tests. In this lesson, we'll focus on unit tests; however, when you start building larger programs, you will want to use integration tests as well.\n",
    "\n",
    "You can read about integration testing and how integration tests relate to unit tests [here](https://www.fullstackpython.com/integration-testing.html). That article contains other very useful links as well.\n",
    "\n",
    "### Unit Testing Tools\n",
    "To install pytest, run pip install -U pytest in your terminal. You can see more information on getting started [here](https://docs.pytest.org/en/latest/getting-started.html).\n",
    "\n",
    "Create a test file starting with test_\n",
    "Define unit test functions that start with test_ inside the test file\n",
    "Enter pytest into your terminal in the directory of your test file and it will detect these tests for you!\n",
    "test_ is the default - if you wish to change this, you can learn how to in this pytest [configuration](https://docs.pytest.org/en/latest/customize.html)\n",
    "\n",
    "In the test output, periods represent successful unit tests and F's represent failed unit tests. Since all you see is what test functions failed, it's wise to have only one assert statement per test. Otherwise, you wouldn't know exactly how many tests failed, and which tests failed.\n",
    "\n",
    "Your tests won't be stopped by failed assert statements, but it will stop if you have syntax errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, make sure to run this command in your terminal to install pytest:\n",
    "```\n",
    "pip install -U pytest\n",
    "```\n",
    "Then, to run pytest, just enter:\n",
    "```\n",
    "pytest\n",
    "```\n",
    "Right now, not all of the tests should pass. Fix the function to pass all its tests! Once all your tests pass, try writing some additional unit tests of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading pytest-5.4.3-py3-none-any.whl (248 kB)\n",
      "\u001b[K     |████████████████████████████████| 248 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (8.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (20.3)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /Applications/anaconda3/lib/python3.7/site-packages (from pytest) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /Applications/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Applications/anaconda3/lib/python3.7/site-packages (from packaging->pytest) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from packaging->pytest) (2.4.7)\n",
      "\u001b[31mERROR: pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n",
      "Installing collected packages: pytest\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 5.4.2\n",
      "    Uninstalling pytest-5.4.2:\n",
      "      Successfully uninstalled pytest-5.4.2\n",
      "Successfully installed pytest-5.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.4, pytest-5.4.3, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /Users/jsr/Documents/GitHub/Software-Engineering\n",
      "plugins: hypothesis-5.16.1, doctestplus-0.7.0, arraydiff-0.3, remotedata-0.3.2, openfiles-0.5.0, astropy-header-0.1.2\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "test_compute_launch.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                              [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m______________________ test_days_until_launch_0_negative _______________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_days_until_launch_0_negative\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m(days_until_launch(\u001b[94m83\u001b[39;49;00m, \u001b[94m64\u001b[39;49;00m) == \u001b[94m0\u001b[39;49;00m)\n",
      "\u001b[1m\u001b[31mE       assert -19 == 0\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where -19 = days_until_launch(83, 64)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_compute_launch.py\u001b[0m:10: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_compute_launch.py::test_days_until_launch_0_negative - assert -19...\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m3 passed\u001b[0m\u001b[31m in 0.07s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Driven Development and Data Science\n",
    "\n",
    "### TEST DRIVEN DEVELOPMENT: \n",
    "Writing tests before you write the code that’s being tested. Your test would fail at first, and you’ll know you’ve finished implementing a task when this test passes.\n",
    "\n",
    "Tests can check for all the different scenarios and edge cases you can think of, before even starting to write your function. This way, when you do start implementing your function, you can run this test to get immediate feedback on whether it works or not in all the ways you can think of, as you tweak your function.\n",
    "When refactoring or adding to your code, tests help you rest assured that the rest of your code didn't break while you were making those changes. Tests also helps ensure that your function behavior is repeatable, regardless of external parameters, such as hardware and time.\n",
    "\n",
    "Test driven development for data science is relatively new and has a lot of experimentation and breakthroughs appearing, which you can learn more about in the resources below.\n",
    "\n",
    "- [Data Science TDD](https://www.linkedin.com/pulse/data-science-test-driven-development-sam-savage/)\n",
    "- [TDD for Data Science](https://engineering.pivotal.io/post/test-driven-development-for-data-science/)\n",
    "- [TDD is Essential for Good Data Science Here's Why](https://medium.com/uk-hydrographic-office/test-driven-development-is-essential-for-good-data-science-heres-why-db7975a03a44)\n",
    "- [Testing Your Code (general python TDD)](https://docs.python-guide.org/writing/tests/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "Logging is valuable for understanding the events that occur while running your program. For example, if you run your model over night and see that it's producing ridiculous results the next day, log messages can really help you understand more about the context in which this occurred. Learn about the qualities that make a log message effective.\n",
    "\n",
    "### Log Messages\n",
    "Logging is the process of recording messages to describe events that have occurred while running your software. Let's take a look at a few examples, and learn tips for writing good log messages.\n",
    "\n",
    "### Tip: Be professional and clear\n",
    "- Bad: Hmmm... this isn't working???\n",
    "- Bad: idk.... :(\n",
    "- __Good: Couldn't parse file.__\n",
    "\n",
    "### Tip: Be concise and use normal capitalization\n",
    "- Bad: Start Product Recommendation Process\n",
    "- Bad: We have completed the steps necessary and will now proceed with the recommendation process for the records in our product database.\n",
    "- __Good: Generating product recommendations.__\n",
    "\n",
    "### Tip: Choose the appropriate level for logging\n",
    "- __DEBUG - level you would use for anything that happens in the program.__\n",
    "- __ERROR - level to record any error that occurs__\n",
    "- __INFO - level to record all actions that are user-driven or system specific, such as regularly scheduled operations__\n",
    "\n",
    "### Tip: Provide any useful information\n",
    "- Bad: Failed to read location data\n",
    "- __Good: Failed to read location data: store_id 8324971__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Reviews\n",
    "Code reviews benefit everyone in a team to promote best programming practices and prepare code for production. Let's go over what to look for in a code review and some tips on how to conduct one.\n",
    "\n",
    "- [Guidelines for code reviews](https://github.com/lyst/MakingLyst/tree/master/code-reviews)\n",
    "- [Code Review Best Practices](https://www.kevinlondon.com/2015/05/05/code-review-best-practices.html)\n",
    "\n",
    "### Questions to Ask Yourself When Conducting a Code Review\n",
    "First, let's look over some of the questions we may ask ourselves while reviewing code. These are simply from the concepts we've covered in these last two lessons!\n",
    "\n",
    "- Is the code clean and modular?\n",
    "- Can I understand the code easily?\n",
    "- Does it use meaningful names and whitespace?\n",
    "- Is there duplicated code?\n",
    "- Can you provide another layer of abstraction?\n",
    "- Is each function and module necessary?\n",
    "- Is each function or module too long?\n",
    "- Is the code efficient?\n",
    "- Are there loops or other steps we can vectorize?\n",
    "- Can we use better data structures to optimize any steps?\n",
    "- Can we shorten the number of calculations needed for any steps?\n",
    "- Can we use generators or multiprocessing to optimize any steps?\n",
    "- Is documentation effective?\n",
    "- Are in-line comments concise and meaningful?\n",
    "- Is there complex code that's missing documentation?\n",
    "- Do function use effective docstrings?\n",
    "- Is the necessary project documentation provided?\n",
    "- Is the code well tested?\n",
    "- Does the code high test coverage?\n",
    "- Do tests check for interesting cases?\n",
    "- Are the tests readable?\n",
    "- Can the tests be made more efficient?\n",
    "- Is the logging effective?\n",
    "- Are log messages clear, concise, and professional?\n",
    "- Do they include all relevant and useful information?\n",
    "- Do they use the appropriate logging level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Conducting a Code Review\n",
    "Now that we know what we are looking for, let's go over some tips on how to actually write your code review. When your coworker finishes up some code that they want to merge to the team's code base, they might send it to you for review. You provide feedback and suggestions, and then they may make changes and send it back to you. When you are happy with the code, you approve and it gets merged to the team's code base.\n",
    "\n",
    "As you may have noticed, with code reviews you are now dealing with people, not just computers. So it's important to be thoughtful of their ideas and efforts. You are in a team and there will be differences in preferences. The goal of code review isn't to make all code follow your personal preferences, but a standard of quality for the whole team.\n",
    "\n",
    "__Tip: Use a code linter__\n",
    "\n",
    "This isn't really a tip for code review, but can save you lots of time from code review! Using a Python code linter like pylint can automatically check for coding standards and PEP 8 guidelines for you! It's also a good idea to agree on a style guide as a team to handle disagreements on code style, whether that's an existing style guide or one you create together incrementally as a team.\n",
    "\n",
    "__Tip: Explain issues and make suggestions__\n",
    "\n",
    "Rather than commanding people to change their code a specific way because it's better, it will go a long way to explain to them the consequences of the current code and suggest changes to improve it. They will be much more receptive to your feedback if they understand your thought process and are accepting recommendations, rather than following commands. They also may have done it a certain way intentionally, and framing it as a suggestion promotes a constructive discussion, rather than opposition.\n",
    "\n",
    "- BAD: Make model evaluation code its own module - too repetitive.\n",
    "\n",
    "- _BETTER:_ Make the model evaluation code its own module. This will simplify models.py to be less repetitive and focus primarily on building models.\n",
    "\n",
    "- __GOOD:__ How about we consider making the model evaluation code its own module? This would simplify models.py to only include code for building models. Organizing these evaluations methods into separate functions would also allow us to reuse them with different models without repeating code.\n",
    "\n",
    "__Tip: Keep your comments objective__\n",
    "\n",
    "Try to avoid using the words \"I\" and \"you\" in your comments. You want to avoid comments that sound personal to bring the attention of the review to the code and not to themselves.\n",
    "\n",
    "- BAD: I wouldn't groupby genre twice like you did here... Just compute it once and use that for your aggregations.\n",
    "\n",
    "- BAD: You create this groupby dataframe twice here. Just compute it once, save it as groupby_genre and then use that to get your average prices and views.\n",
    "\n",
    "- __GOOD:__ Can we group by genre at the beginning of the function and then save that as a groupby object? We could then reference that object to get the average prices and views without computing groupby twice.\n",
    "\n",
    "__Tip: Provide code examples__\n",
    "\n",
    "When providing a code review, you can save the author time and make it easy for them to act on your feedback by writing out your code suggestions. This shows you are willing to spend some extra time to review their code and help them out. It can also just be much quicker for you to demonstrate concepts through code rather than explanations.\n",
    "\n",
    "Let's say you were reviewing code that included the following lines:\n",
    "\n",
    "```python\n",
    "first_names = []\n",
    "last_names = []\n",
    "\n",
    "for name in enumerate(df.name):\n",
    "    first, last = name.split(' ')\n",
    "    first_names.append(first)\n",
    "    last_names.append(last)\n",
    "\n",
    "df['first_name'] = first_names\n",
    "df['last_names'] = last_names\n",
    "```\n",
    "\n",
    "- BAD: You can do this all in one step by using the pandas str.split method.\n",
    "\n",
    "__GOOD:__ We can actually simplify this step to the line below using the pandas str.split method. Found this on this [stack overflow post](https://stackoverflow.com/questions/14745022/how-to-split-a-column-into-two-columns)\n",
    "\n",
    "```python\n",
    "df['first_name'], df['last_name'] = df['name'].str.split(' ', 1).str\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
